{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 with Gabor filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "weights = None  # 'imagenet'\n",
    "input_shape = (224, 224, 3)  # (224, 224, 16)\n",
    "classes = 10\n",
    "model = VGG16(include_top=True, weights=weights, input_tensor=None, input_shape=input_shape, pooling=None, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer\n",
    "model.layers.pop(0)\n",
    "gabor_input = InputLayer(input_shape=(224, 224, 32))\n",
    "gabor_output = model(gabor_input)\n",
    "gabor_model = Model(gabor_input, gabor_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_kernel_tensor(ksize, sigmas, thetas, lambdas, gammas, psis):\n",
    "#     size = (25, 25)  # [(5, 5), (15, 15), (25, 25)]\n",
    "#     sigmas = [2, 4]\n",
    "#     thetas = np.linspace(0, 2*np.pi, 8, endpoint=False)  # [0, np.pi/4, np.pi/2, np.pi*3/4]\n",
    "#     lambdas = [2, 4, 8, 16]\n",
    "#     psi = np.pi/2\n",
    "#     gamma = 0.5\n",
    "    n_kernels = len(sigmas) * len(thetas) * len(lambdas) * len(gammas)\n",
    "    gabors = []\n",
    "    for sigma in sigmas:\n",
    "        for theta in thetas:\n",
    "            for lambd in lambdas:\n",
    "                for gamma in gammas:\n",
    "                    for psi in psis:\n",
    "                        params = {'ksize': ksize, 'sigma': sigma, 'theta': theta, 'lambd': lambd, 'gamma': gamma, 'psi': psi}\n",
    "                        gf = cv2.getGaborKernel(**params, ktype=cv2.CV_32F)\n",
    "                        gf = K.expand_dims(gf, -1)\n",
    "                        gabors.append(gf)\n",
    "    assert len(gabors) == n_kernels\n",
    "    print(f\"Created {n_kernels} kernels.\")\n",
    "    return K.stack(gabors, axis=-1)\n",
    "#     print(img.shape)\n",
    "#     print(gft.shape)\n",
    "\n",
    "ksize = (25, 25)\n",
    "sigmas = [2, 4]\n",
    "thetas = np.linspace(0, 2*np.pi, 8, endpoint=False)  # [0, np.pi/4, np.pi/2, np.pi*3/4]\n",
    "lambdas = [2, 4, 8, 16]\n",
    "psis = [np.pi/2]\n",
    "gammas = [0.5]\n",
    "gft = get_kernel_tensor(ksize, sigmas, thetas, lambdas, gammas, psis)\n",
    "\n",
    "def gabor_filter(x, kernel_tensor=None):\n",
    "    '''\n",
    "    conv2d\n",
    "    input tensor of shape [batch, in_height, in_width, in_channels]\n",
    "    kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    '''\n",
    "    x = tf.image.rgb_to_grayscale(x)\n",
    "    return K.conv2d(x, kernel_tensor, padding='same')\n",
    "\n",
    "def lambda_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "# gabor_layer = Lambda(gabor_filter, mask=None, arguments={'kernel_tensor': gft})  #, output_shape=lambda_output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [l for l in model.layers]\n",
    "x = layers[0].output\n",
    "x = Lambda(gabor_filter, arguments={'kernel_tensor': gft})(x)\n",
    "for l in range(2, len(layers)):\n",
    "    x = layers[l](x)\n",
    "\n",
    "GaborNet = Model(inputs=layers[0].input, outputs=x)\n",
    "GaborNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints, metrics\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Dense, Dropout, Activation, Flatten, Reshape, Layer,\n",
    "                          BatchNormalization, LocallyConnected2D,\n",
    "                          ZeroPadding2D, Conv2D, MaxPooling2D, Conv2DTranspose,\n",
    "                          GaussianNoise, UpSampling2D, Input)\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.utils import conv_utils, multi_gpu_model\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "from keras.layers import Lambda\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "def get_kernel_tensor(ksize, sigmas, thetas, lambdas, gammas, psis):\n",
    "\n",
    "    n_kernels = len(sigmas) * len(thetas) * len(lambdas) * len(gammas)\n",
    "    gabors = []\n",
    "    for sigma in sigmas:\n",
    "        for theta in thetas:\n",
    "            for lambd in lambdas:\n",
    "                for gamma in gammas:\n",
    "                    for psi in psis:\n",
    "                        params = {'ksize': ksize, 'sigma': sigma, 'theta': theta, 'lambd': lambd, 'gamma': gamma, 'psi': psi}\n",
    "                        gf = cv2.getGaborKernel(**params, ktype=cv2.CV_32F)\n",
    "                        gf = K.expand_dims(gf, -1)\n",
    "                        gabors.append(gf)\n",
    "    assert len(gabors) == n_kernels\n",
    "    print(f\"Created {n_kernels} kernels.\")\n",
    "    return K.stack(gabors, axis=-1)\n",
    "\n",
    "\n",
    "def gabor_filter(x, kernel_tensor=None):\n",
    "    '''\n",
    "    conv2d\n",
    "    input tensor of shape [batch, in_height, in_width, in_channels]\n",
    "    kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    '''\n",
    "    # x = tf.image.rgb_to_grayscale(x)\n",
    "    return K.conv2d(x, kernel_tensor, padding='same')\n",
    "\n",
    "\n",
    "def lambda_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "\n",
    "weights = None  # 'imagenet'\n",
    "input_shape = (32, 32, 1) # (224, 224, 3)  # (224, 224, 16)\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'results')  # TODO: /workspace/results\n",
    "data_set = 'pixel'\n",
    "data_root = '/workspace/data/pixel/small'  # TODO: Pass in\n",
    "stimulus_set = 'jitter'  # 'static'  # 'set_32_32'\n",
    "noise_types = ['Original', 'Salt-and-pepper', 'Additive', 'Single-pixel']\n",
    "pretrained_model = False\n",
    "data_augmentation = False\n",
    "task = 'classification'\n",
    "fresh_data = True\n",
    "n_gpus = 1\n",
    "epochs = 20\n",
    "\n",
    "# Gabor filter parameters\n",
    "ksize = (25, 25)\n",
    "sigmas = [2, 4]\n",
    "thetas = np.linspace(0, 2*np.pi, 8, endpoint=False)  # [0, np.pi/4, np.pi/2, np.pi*3/4]\n",
    "lambdas = [8, 16, 32, 64]\n",
    "psis = [np.pi/2]\n",
    "gammas = [0.5]\n",
    "\n",
    "# fresh_data = True\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "\n",
    "for noise_type in noise_types:\n",
    "\n",
    "    model_name = f\"{data_set}_{stimulus_set}_{noise_type}\"\n",
    "    print(model_name)\n",
    "\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "\n",
    "    # if data_set == 'cifar10':\n",
    "    #     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    #     x_train = np.mean(x_train, 3, keepdims=True)  # Average over RGB channels\n",
    "    #     x_test = np.mean(x_test, 3, keepdims=True)  # Average over RGB channels\n",
    "    #     x_train = x_train.astype('float32')\n",
    "    #     x_test = x_test.astype('float32')\n",
    "    #     x_train /= 255\n",
    "    #     x_test /= 255\n",
    "\n",
    "    #     # Convert class vectors to binary class matrices.\n",
    "    #     y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    #     y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    # elif data_set == 'pixel':\n",
    "\n",
    "\n",
    "    test_conditions = ['Same', 'Diff', 'NoPix']\n",
    "\n",
    "    #   (noise_type, trial) = trial_label.split(\"_\")\n",
    "\n",
    "    if noise_type == 'Original':\n",
    "        data_path = os.path.join(data_root, stimulus_set, 'orig')\n",
    "    elif noise_type == 'Salt-and-pepper':\n",
    "        data_path = os.path.join(data_root, stimulus_set, 'salt_n_pepper')\n",
    "    elif noise_type == 'Additive':\n",
    "        data_path = os.path.join(data_root, stimulus_set, 'uniform')\n",
    "    elif noise_type == 'Single-pixel':\n",
    "        data_path = os.path.join(data_root, stimulus_set, 'single_pixel')\n",
    "    else:\n",
    "        sys.exit(f\"Unknown noise type requested: {noise_type}\")\n",
    "\n",
    "    def load_images(path):\n",
    "\n",
    "        image_set = {}\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            if root == path:\n",
    "                categories = sorted(dirs)\n",
    "                image_set = {cat: [] for cat in categories}\n",
    "            else:\n",
    "                image_set[os.path.basename(root)] = sorted(files)\n",
    "\n",
    "        n_cat_images = {cat: len(files) for (cat, files) in image_set.items()}\n",
    "        n_images = sum(n_cat_images.values())\n",
    "        image_dims = plt.imread(os.path.join(path, categories[0],\n",
    "                                image_set[categories[0]][0])).shape\n",
    "\n",
    "        print(image_dims)\n",
    "        # X = np.zeros((n_images, *image_dims), dtype='float32')\n",
    "        X = np.zeros((n_images, image_dims[0], image_dims[1], 1), dtype='float32')\n",
    "        y = np.zeros((n_images, len(categories)), dtype=int)\n",
    "        # y = np.zeros(n_images, dtype=int)\n",
    "\n",
    "        tally = 0\n",
    "        for c, (cat, files) in enumerate(tqdm(image_set.items(), desc=path)):\n",
    "            for i, image in enumerate(files):\n",
    "                cimg = plt.imread(os.path.join(path, cat, image))\n",
    "                X[i+tally] = np.expand_dims(cv2.cvtColor(cimg, cv2.COLOR_BGR2GRAY), axis=-1)\n",
    "            y[tally:tally+len(files), c] = True\n",
    "            tally += len(files)\n",
    "\n",
    "        shuffle = np.random.permutation(y.shape[0])\n",
    "\n",
    "        return image_set, X[shuffle], y[shuffle]\n",
    "\n",
    "    train_path = os.path.join(data_path, 'train')\n",
    "    # test_path = os.path.join(data_path, f\"test_{noise_cond.lower()}\")\n",
    "\n",
    "    if os.path.isfile(os.path.join(train_path, 'x_train.npy')) and not fresh_data:\n",
    "        print(f'Loading {data_set} data arrays.')\n",
    "        x_train = np.load(os.path.join(train_path, 'x_train.npy'))\n",
    "        y_train = np.load(os.path.join(train_path, 'y_train.npy'))\n",
    "        # num_classes = len(os.listdir(train_path)) - 1\n",
    "        cat_dirs = [os.path.join(train_path, o) for o in os.listdir(train_path)\n",
    "                    if os.path.isdir(os.path.join(train_path, o))]\n",
    "        assert num_classes == len(cat_dirs)\n",
    "    else:\n",
    "        print(f'Loading {data_set} image files.')\n",
    "        train_images, x_train, y_train = load_images(train_path)\n",
    "        print(train_images.keys())\n",
    "        assert num_classes == len(train_images)\n",
    "        np.save(os.path.join(train_path, 'x_train.npy'), x_train)\n",
    "        np.save(os.path.join(train_path, 'y_train.npy'), y_train)\n",
    "\n",
    "    # x_train = np.mean(x_train, 3, keepdims=True)  # Average over RGB channels\n",
    "\n",
    "    test_sets = []\n",
    "    for test_cond in test_conditions:\n",
    "        test_path = os.path.join(data_path, f\"test_{test_cond.lower()}\")\n",
    "        if os.path.isfile(os.path.join(test_path, 'x_test.npy')) and not fresh_data:\n",
    "            x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n",
    "            y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n",
    "        else:\n",
    "            test_images, x_test, y_test = load_images(test_path)\n",
    "            print(test_images.keys())\n",
    "            assert num_classes == len(test_images)\n",
    "            np.save(os.path.join(test_path, 'x_test.npy'), x_test)\n",
    "            np.save(os.path.join(test_path, 'y_test.npy'), y_test)\n",
    "        # test_sets.append((np.mean(x_test, 3, keepdims=True), y_test))\n",
    "        test_sets.append((x_test, y_test))\n",
    "    test_cond = \"NoPix\"  # Use this for examining learning curves\n",
    "    x_test, y_test = test_sets[test_conditions.index(\"NoPix\")]  # Unpack default test set\n",
    "    # else:\n",
    "    #     sys.exit(f\"Unknown data set requested: {data_set}\")\n",
    "\n",
    "    # Summarise stimuli\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print(y_train.shape[1], 'training categories')\n",
    "    print(y_test.shape[1], 'testing categories')\n",
    "\n",
    "\n",
    "    # Import VGG16\n",
    "    model = VGG16(include_top=True, weights=weights, input_tensor=None, input_shape=input_shape, pooling=None, classes=num_classes)\n",
    "\n",
    "    # Generate Gabor filters\n",
    "    gft = get_kernel_tensor(ksize, sigmas, thetas, lambdas, gammas, psis)\n",
    "\n",
    "    # Modify standard VGG16 with hardcoded Gabor convolutional layer\n",
    "    layers = [l for l in model.layers]\n",
    "    # x = layers[0].output\n",
    "    inp = Input(shape=x_train[0].shape)\n",
    "    x = Lambda(gabor_filter, arguments={'kernel_tensor': gft})(inp)\n",
    "    for l in range(2, len(layers)):\n",
    "        x = layers[l](x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # model.summary()\n",
    "\n",
    "\n",
    "    if pretrained_model:\n",
    "        # Load weights from saved model\n",
    "        pretrained_model_path = os.path.join(save_dir, pretrained_model)\n",
    "        model.load_weights(pretrained_model_path, by_name=True)\n",
    "\n",
    "        # Freeze weights in convolutional layers during training\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, keras.layers.convolutional.Conv2D):\n",
    "                print(f\"Freezing layer: {layer.name}\")\n",
    "                layer.trainable = False\n",
    "\n",
    "    if n_gpus > 1:\n",
    "        model = multi_gpu_model(model, gpus=n_gpus)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "    # Compile the model last before training for all changes to take effect\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        if task == 'classification':\n",
    "            hist = model.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True)\n",
    "\n",
    "    else:\n",
    "        print('Using data augmentation.')\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            rotation_range=0,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=False)\n",
    "\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        if task == 'classification':\n",
    "            hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                                    batch_size=batch_size),\n",
    "                                       steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n",
    "                                       epochs=epochs,\n",
    "                                       validation_data=(x_test, y_test),\n",
    "                                       workers=4)\n",
    "\n",
    "    print('History', hist.history)\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    # model_name = 'SAVED'+'_'+model_name\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    # model.save(model_path)\n",
    "    np.save(os.path.join(save_dir, f'{model_name}_VALACC.npy'), hist.history['val_acc'])\n",
    "    np.save(os.path.join(save_dir, f'{model_name}_ACC.npy'), hist.history['acc'])\n",
    "    np.save(os.path.join(save_dir, f'{model_name}_VALLOSS.npy'), hist.history['val_loss'])\n",
    "    np.save(os.path.join(save_dir, f'{model_name}_LOSS.npy'), hist.history['loss'])\n",
    "\n",
    "    if data_set == 'pixel':\n",
    "        cond_acc = {}\n",
    "        cond_loss = {}\n",
    "        for test_cond, (x_test, y_test) in zip(test_conditions, test_sets):\n",
    "            loss, val_acc = model.evaluate(x=x_test, y=y_test, batch_size=batch_size)\n",
    "            cond_acc[test_cond] = val_acc\n",
    "            cond_loss[test_cond] = loss\n",
    "        print(\"Saving metrics: \", model.metrics_names)\n",
    "        with open(os.path.join(save_dir, f'{model_name}_CONDVALACC.json'), \"w\") as jf:\n",
    "            json.dump(cond_acc, jf)\n",
    "        with open(os.path.join(save_dir, f'{model_name}_CONDVALLOSS.json'), \"w\") as jf:\n",
    "            json.dump(cond_loss, jf)\n",
    "\n",
    "    print(f'Saved trained model at {model_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy scores and plot\n",
    "\n",
    "%matplotlib inline\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Defaults\n",
    "# filter_size = 9\n",
    "# retina_layers = 2\n",
    "# vvs_width = 32\n",
    "\n",
    "# data_set = 'pixel'\n",
    "# stimulus_set = 'static'\n",
    "n_epochs = 20\n",
    "num_trials = 1\n",
    "noise_types = ['Original', 'Salt-and-pepper', 'Additive', 'Single-pixel']  # 'Original'\n",
    "\n",
    "\n",
    "rows = []\n",
    "test_rows = []\n",
    "\n",
    "for trial in range(1, 1+num_trials):\n",
    "    for noise_type in noise_types:\n",
    "#         trial_label = f\"{noise_type}_{trial}\"\n",
    "#         model_name = f\"{data_set}_{trial_label}_vvs_layers={vvs_layers}_retina_out_channels={retina_out_width}\"\n",
    "\n",
    "        model_name = f\"{data_set}_{stimulus_set}_{noise_type}\"\n",
    "        acc_scores = np.load(os.path.join('results', f'{model_name}_ACC.npy'))\n",
    "        valacc_scores = np.load(os.path.join('results', f'{model_name}_VALACC.npy'))\n",
    "        loss = np.load(os.path.join('results', f'{model_name}_LOSS.npy'))\n",
    "        valloss = np.load(os.path.join('results', f'{model_name}_VALLOSS.npy'))\n",
    "\n",
    "        with open(os.path.join('results', f'{model_name}_CONDVALACC.json'), \"r\") as jf:\n",
    "            cond_acc = json.load(jf)\n",
    "        with open(os.path.join('results', f'{model_name}_CONDVALLOSS.json'), \"r\") as jf:\n",
    "            cond_loss = json.load(jf)\n",
    "\n",
    "        for condition in test_conditions:\n",
    "            test_rows.append({'Trial': trial, 'Noise Type': noise_type,\n",
    "                             'Condition': condition, 'Loss': cond_loss[condition], \n",
    "                              'Accuracy': cond_acc[condition]})\n",
    "        for epoch in range(n_epochs):\n",
    "            rows.append({'Trial': trial, 'Noise Type': noise_type, #'Noise Condition': noise_cond, \n",
    "                         'Evaluation': 'Testing', 'Epoch': epoch+1, 'Loss': valloss[epoch], \n",
    "                         'Accuracy': valacc_scores[epoch]})\n",
    "\n",
    "            rows.append({'Trial': trial, 'Noise Type': noise_type, #'Noise Condition': noise_cond, \n",
    "                         'Evaluation': 'Training', 'Epoch': epoch+1, 'Loss': loss[epoch], \n",
    "                         'Accuracy': acc_scores[epoch]})\n",
    "\n",
    "scores = pd.DataFrame(rows, columns=['Trial', 'Noise Type', 'Evaluation', 'Epoch', 'Loss', 'Accuracy'])\n",
    "test_scores = pd.DataFrame(test_rows, columns=['Trial', 'Noise Type', 'Condition', 'Loss', 'Accuracy'])\n",
    "# scores\n",
    "# test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# g = sns.relplot(x='Epoch', y='Accuracy', style='Retina Out Width', hue='Evaluation', row='VVS Layers', col='Noise Type', kind='line', data=scores)\n",
    "g = sns.relplot(x='Epoch', y='Accuracy', hue='Evaluation', col='Noise Type', kind='line', data=scores)\n",
    "# g = sns.relplot(x='Epoch', y='Loss', style='Retina Out Width', hue='Evaluation', row='VVS Layers', col='Noise Type', kind='line', data=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "# g = sns.catplot(x=\"Condition\", y=\"Accuracy\", hue=\"Retina Out Width\", row=\"VVS Layers\", col=\"Noise Type\", kind=\"bar\", data=test_scores)\n",
    "g = sns.catplot(x=\"Condition\", y=\"Accuracy\", col=\"Noise Type\", kind=\"bar\", data=test_scores)\n",
    "# g = sns.catplot(x=\"Condition\", y=\"Loss\", hue=\"Retina Out Width\", row=\"VVS Layers\", col=\"Noise Type\", kind=\"bar\", data=test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import InputLayer\n",
    "model.layers.pop(1)  # First convolutional layer\n",
    "gabor_input = InputLayer(input_shape=(224, 224, 32))\n",
    "gabor_output = model(gabor_input)\n",
    "gabor_model = Model(gabor_input, gabor_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# import cv2\n",
    "# from keras import backend as K\n",
    "# import numpy as np\n",
    "\n",
    "image_path = \"/workspace/data/Lenna.png\"\n",
    "# img = image.load_img(image_path)\n",
    "img = plt.imread(image_path)\n",
    "print(img.shape)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "print(type(img))\n",
    "img = K.expand_dims(img, 0)\n",
    "img = K.expand_dims(img, -1)\n",
    "\n",
    "gabor_filter(img, kernel_tensor=gft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session().as_default():\n",
    "    plt.imshow(gabor_filter(img, kernel_tensor=gft).eval()[0,:,:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gabor_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='vgg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
